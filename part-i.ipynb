{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3efce6395bf1641",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Why you should care? ü§ì](#Why-you-should-care?-%F0%9F%A4%93)\n",
    "- [Why Was AI Developed? ü§î](#Why-Was-AI-Developed?-%F0%9F%A4%94)\n",
    "- [When Did It All Start? üï∞Ô∏è](#When-Did-It-All-Start?-%F0%9F%95%B0%EF%B8%8F)\n",
    "- [Key Stages of Development and Technical Details üöÄ](#Key-Stages-of-Development-and-Technical-Details-%F0%9F%9A%80)\n",
    "  - [Early Enthusiasm & \"Good Old-Fashioned AI\" (GOFAI) - 1950s-1980s üí°](#Early-Enthusiasm-&-%22Good-Old-Fashioned-AI%22-(GOFAI)---1950s-1980s-%F0%9F%92%A1)\n",
    "  - [AI Winters - 1970s & 1980s ‚ùÑÔ∏è](#AI-Winters---1970s-&-1980s-%E2%9D%84%EF%B8%8F)\n",
    "  - [The Rise of Machine Learning - 1990s-Early 2010s üå±](#The-Rise-of-Machine-Learning---1990s-Early-2010s-%F0%9F%8C%B1)\n",
    "  - [The Deep Learning Revolution & Current State - 2010s-Today üåü](#The-Deep-Learning-Revolution-&-Current-State---2010s-Today-%F0%9F%8C%9F)\n",
    "    - [Key techniques](#Key-techniques:)\n",
    "    - [Impact](#Impact:)\n",
    "- [How AI got here?](#How-AI-got-here?)\n",
    "  - [Core Concepts of ML](#Core-Concepts-of-ML)\n",
    "    - [Learning](#Learning)\n",
    "    - [Structured vs. Unstructured Data](#Structured-vs.-Unstructured-Data)\n",
    "    - [Supervised Learning](#Supervised-Learning)\n",
    "- [Introduction to Neural Networks & Deep Learning: The AI \"Brain\" üß†](#Introduction-to-Neural-Networks-&-Deep-Learning:-The-AI-%22Brain%22-%F0%9F%A7%A0)\n",
    "  - [The Artificial Neuron: The Basic Building Block ‚ö°](#The-Artificial-Neuron:-The-Basic-Building-Block-%E2%9A%A1)\n",
    "  - [Neural Networks: Connecting the Neurons in Layers üîó](#Neural-Networks:-Connecting-the-Neurons-in-Layers-%F0%9F%94%97)\n",
    "    - [Deep Learning: More Layers, More Power! üöÄ](#Deep-Learning:-More-Layers,-More-Power!-%F0%9F%9A%80)\n",
    "    - [Key Architectures üèóÔ∏è](#Key-Architectures-%F0%9F%8F%97%EF%B8%8F)\n",
    "- [Modern LLMs and why all of the above matter ü§ñ](#Modern-LLMs-and-why-all-of-the-above-matter-%F0%9F%A4%96)\n",
    "  - [Large Language Models (LLMs)](#Large-Language-Models-(LLMs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad6da1696eea0af",
   "metadata": {},
   "source": [
    "# Why you should care? ü§ì\n",
    "AI is a once-in-a-generation technology, or a [General Purpose Technology](https://en.wikipedia.org/wiki/General-purpose_technology) (ironically, GPT).\n",
    "\n",
    "\"Fun\" facts or **Race of the Century**:\n",
    "\n",
    "- US plans to spend \\$1trn by 2030 on data centers for AI models\n",
    "- Liang Wenfeng, DeepSeek's founder, has made developing AGI (Artificial General intelligence) his firm's mission\n",
    "- DeepSeek promised to spend around \\$53bn over the next 3 years to build data centres to meet demand for AI cloud services\n",
    "- SoftBank injected \\$40bn into OpenAI\n",
    "- Market Capitalisation of ASML (Europe's manufacturer of semiconductors / lithography tools) raised from <‚Ç¨100bn to ‚Ç¨300bn during last 5 years\n",
    "\n",
    "*A nice article about [AGI](https://www.ibm.com/think/topics/artificial-general-intelligence)*\n",
    "\n",
    "# Why Was AI Developed? ü§î\n",
    "\n",
    "- automate tasks\n",
    "- solve complex problems\n",
    "- extend our capabilities\n",
    "\n",
    "# When Did It All Start? üï∞Ô∏è\n",
    "\n",
    "- 1940s-1950s: The Dawn of AI üåÖ\n",
    "    - *\"Turing Test\"* (1950) by Alan Turing.\n",
    "    Recent AI systems easily pass the test. However, their \"passing\" is [still debated](https://roboticsandautomationnews.com/2024/12/18/the-turing-test-origins-significance-and-controversies/87764/).\n",
    "- *The Dartmouth Workshop* (1956). The term \"Artificial Intelligence\" is officially coined and is widely considered the birth of AI as a distinct academic discipline.\n",
    "\n",
    "# Key Stages of Development and Technical Details üöÄ\n",
    "\n",
    "## Early Enthusiasm & *\"Good Old-Fashioned AI\"* (GOFAI) - 1950s-1980s üí°\n",
    "Symbolic AI, where intelligence was represented using symbols and rules, much like a decision tree. This is called \"hard-coding\".\n",
    "## AI Winters - 1970s & 1980s ‚ùÑÔ∏è\n",
    "Periods of reduced funding and interest due to unmet expectations and the limitations of GOFAI.\n",
    "## The Rise of Machine Learning - 1990s-Early 2010s üå±\n",
    "Switch from hard-coding approach to machines learning from data.\n",
    "\n",
    "These include:\n",
    "- Decision Trees\n",
    "- Support Vector Machines\n",
    "- Neural Networks\n",
    "\n",
    "## The Deep Learning Revolution & Current State - 2010s-Today üåü\n",
    "Huge advancements in computational power (GPUs), vast amounts of data (\"Big Data\"), and new algorithms.\n",
    "\n",
    "### Key techniques:\n",
    "- Convolutional Neural Networks\n",
    "- Recurrent Neural Networks / Transformers\n",
    "\n",
    "### Impact:\n",
    "- Natural Language processing (NLP)\n",
    "- Image Recognition\n",
    "- Speach Recognition\n",
    "- Driving Vehicles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc697d740433123",
   "metadata": {},
   "source": [
    "# How AI got here?\n",
    "\n",
    "How these intelligent systems actually operate?\n",
    "\n",
    "## Core Concepts of ML\n",
    "\n",
    "### Learning\n",
    "\n",
    "Using structured (table) or unstructured (text, image, audio) data to train a model.\n",
    "\n",
    "Learning typically involves:\n",
    "\n",
    "- *Feeding Data*: Giving the AI a large collection of relevant information.\n",
    "- *Identifying Features*: The AI (or its human trainer) identifies specific characteristics or measurements within that data that might be important for finding patterns.\n",
    "- *Pattern Recognition*: The AI uses mathematical algorithms to analyze these features and discover hidden relationships or rules.\n",
    "- *Making Predictions/Decisions*: Once trained, the AI can then apply the patterns it learned to new, unseen data to make predictions or decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0ec402b37e54d",
   "metadata": {},
   "source": [
    "### Structured vs. Unstructured Data\n",
    "\n",
    "- **Structured Data**: Data that is highly organized and follows a predefined format. It's typically found in tables, where each column represents a specific attribute (feature) and each row represents a distinct record or observation. It's easy for computers to read and process directly.\n",
    "- **Unstructured Data**: Data that does not have a predefined structure. It's typically human-readable but difficult for traditional computers to parse and understand directly. It requires more advanced AI techniques (like Deep Learning) to extract meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a771cf74aa3b201d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T12:46:32.414651Z",
     "start_time": "2025-06-17T12:46:32.087154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mineral Composition Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample ID</th>\n",
       "      <th>Quartz (%)</th>\n",
       "      <th>Feldspar (%)</th>\n",
       "      <th>Mica (%)</th>\n",
       "      <th>Pyrite (%)</th>\n",
       "      <th>Rock Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G-001</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>Granite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G-002</td>\n",
       "      <td>20</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>Diorite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G-003</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Quartzite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G-004</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>Schist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sample ID  Quartz (%)  Feldspar (%)  Mica (%)  Pyrite (%)  Rock Type\n",
       "0     G-001          35            40        15           5    Granite\n",
       "1     G-002          20            60        10           2    Diorite\n",
       "2     G-003          80             5         5           0  Quartzite\n",
       "3     G-004          50            30        15           1     Schist"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of structured table for ML learning\n",
    "import pandas as pd\n",
    "\n",
    "mineral_data = {\n",
    "    'Sample ID': ['G-001', 'G-002', 'G-003', 'G-004'],\n",
    "    'Quartz (%)': [35, 20, 80, 50],\n",
    "    'Feldspar (%)': [40, 60, 5, 30],\n",
    "    'Mica (%)': [15, 10, 5, 15],\n",
    "    'Pyrite (%)': [5, 2, 0, 1],\n",
    "    'Rock Type': ['Granite', 'Diorite', 'Quartzite', 'Schist']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(mineral_data)\n",
    "\n",
    "print(\"Mineral Composition Data:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "721266ffcccb36bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T08:57:38.312217Z",
     "start_time": "2025-06-17T08:57:38.304181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Unstructured Data Example: Core Log Annotations (JSON) ---\n",
      "Type of data: <class 'list'>\n",
      "Number of core segments recorded: 3\n",
      "\n",
      "Full JSON Structure:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'segment_id': 'C-001-A',\n",
       "  'depth_interval_m': {'start': 10.0, 'end': 12.5},\n",
       "  'lithology_description': 'Massive, coarse-grained granite, pinkish-grey, few sparse dark minerals. Occasional quartz veins.',\n",
       "  'alteration': [{'type': 'Sericitic',\n",
       "    'intensity': 'moderate',\n",
       "    'location': 'fractures'},\n",
       "   {'type': 'Chloritic', 'intensity': 'weak', 'location': 'mafic minerals'}],\n",
       "  'structures': ['Jointing (vertical)', 'Rare micro-fractures'],\n",
       "  'minerals_present': ['Quartz', 'Feldspar', 'Biotite', 'Muscovite'],\n",
       "  'field_notes': 'No visible sulfides.'},\n",
       " {'segment_id': 'C-001-B',\n",
       "  'depth_interval_m': {'start': 12.5, 'end': 13.0},\n",
       "  'lithology_description': 'Sheared zone with highly fractured pegmatite. Abundant secondary mineralization.',\n",
       "  'alteration': [{'type': 'Potassic', 'intensity': 'strong'},\n",
       "   {'type': 'Silicification', 'intensity': 'pervasive'}],\n",
       "  'structures': ['Mylonitic fabric', 'Brecciation'],\n",
       "  'mineral_occurrence': {'chalcopyrite': {'concentration': 'high',\n",
       "    'form': 'disseminated',\n",
       "    'min_size_mm': 0.5,\n",
       "    'max_size_mm': 2.0},\n",
       "   'pyrite': {'concentration': 'moderate', 'form': 'veinlets'}},\n",
       "  'geochemical_notes': 'High gamma-ray readings.'},\n",
       " {'segment_id': 'C-001-C',\n",
       "  'depth_interval_m': {'start': 13.0, 'end': 15.0},\n",
       "  'lithology_description': 'Fine-grained basalt with vesicular texture. Occasional amygdules filled with calcite.',\n",
       "  'structures': ['Columnar jointing (conceptual)'],\n",
       "  'porosity_percent': 5.2}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of structured table for ML learning\n",
    "core_log_data = [\n",
    "    {\n",
    "        \"segment_id\": \"C-001-A\",\n",
    "        \"depth_interval_m\": {\"start\": 10.0, \"end\": 12.5},\n",
    "        \"lithology_description\": \"Massive, coarse-grained granite, pinkish-grey, few sparse dark minerals. Occasional quartz veins.\",\n",
    "        \"alteration\": [\n",
    "            {\"type\": \"Sericitic\", \"intensity\": \"moderate\", \"location\": \"fractures\"},\n",
    "            {\"type\": \"Chloritic\", \"intensity\": \"weak\", \"location\": \"mafic minerals\"}\n",
    "        ],\n",
    "        \"structures\": [\"Jointing (vertical)\", \"Rare micro-fractures\"],\n",
    "        \"minerals_present\": [\"Quartz\", \"Feldspar\", \"Biotite\", \"Muscovite\"],\n",
    "        \"field_notes\": \"No visible sulfides.\"\n",
    "    },\n",
    "    {\n",
    "        \"segment_id\": \"C-001-B\",\n",
    "        \"depth_interval_m\": {\"start\": 12.5, \"end\": 13.0},\n",
    "        \"lithology_description\": \"Sheared zone with highly fractured pegmatite. Abundant secondary mineralization.\",\n",
    "        \"alteration\": [\n",
    "            {\"type\": \"Potassic\", \"intensity\": \"strong\"},\n",
    "            {\"type\": \"Silicification\", \"intensity\": \"pervasive\"}\n",
    "        ],\n",
    "        \"structures\": [\"Mylonitic fabric\", \"Brecciation\"],\n",
    "        \"mineral_occurrence\": {\n",
    "            \"chalcopyrite\": {\"concentration\": \"high\", \"form\": \"disseminated\", \"min_size_mm\": 0.5, \"max_size_mm\": 2.0},\n",
    "            \"pyrite\": {\"concentration\": \"moderate\", \"form\": \"veinlets\"}\n",
    "        },\n",
    "        \"geochemical_notes\": \"High gamma-ray readings.\"\n",
    "    },\n",
    "    {\n",
    "        \"segment_id\": \"C-001-C\",\n",
    "        \"depth_interval_m\": {\"start\": 13.0, \"end\": 15.0},\n",
    "        \"lithology_description\": \"Fine-grained basalt with vesicular texture. Occasional amygdules filled with calcite.\",\n",
    "        # Note: 'alteration' and 'mineral_occurrence' are missing for this segment,\n",
    "        # demonstrating the flexible nature of unstructured data.\n",
    "        \"structures\": [\"Columnar jointing (conceptual)\"],\n",
    "        \"porosity_percent\": 5.2\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n--- Unstructured Data Example: Core Log Annotations (JSON) ---\")\n",
    "print(\"Type of data:\", type(core_log_data))\n",
    "print(\"Number of core segments recorded:\", len(core_log_data))\n",
    "\n",
    "core_log_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a042fdf141144af",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "Imagine looking at thousands of thin sections and labeling each one with its rock type, dominant mineral, or presence of ore minerals. The AI then learns from these labeled examples.\n",
    "\n",
    "Examples:\n",
    "- Classification: Predicting a category or class (e.g., igneous/volcanic, granite/basalt).\n",
    "- Regression: Predicting continuous values (e.g., silica saturation, iron content).\n",
    "\n",
    "Practical Examples:\n",
    "- **Mineral Identification from Hyperspectral Data** (Classification).\n",
    "    - Input Data: Hyperspectral reflectance curves (unstructured data, but each curve is tied to a specific sample).\n",
    "    - Labels: The known mineral present in that sample (e.g., \"Quartz,\" \"Calcite,\" \"Pyroxene\").\n",
    "    - How AI Learns: The AI is shown many spectral curves, and for each curve, it's told, \"This curve belongs to Quartz,\" or \"This one is Calcite.\" It learns the unique spectral \"fingerprint\" of each mineral.\n",
    "    - Application: Given a new, unlabeled hyperspectral curve from a field scan or a core, the AI can predict which mineral it most likely is. This helps in rapid mineral mapping.\n",
    "- **Predicting Ore Grade from Geochemical Assays** (Regression)\n",
    "    - Input Data: Geochemical concentrations of various elements in drill core samples (e.g., Fe, Mg, Al, K, Na, S). This is structured data.\n",
    "    - Labels: The actual measured concentration of a target metal (e.g., Au in ppm, Cu in %) for each sample.\n",
    "    - How AI Learns: The AI is given thousands of geochemical profiles along with their corresponding gold or copper grades. It learns complex, often non-linear relationships between the presence and concentration of other elements and the target metal's grade.\n",
    "    - Application: For new drill core samples where a full assay might be expensive or slow, AI can provide a quick, estimated ore grade based on a cheaper, faster geochemical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee873501fc002a",
   "metadata": {},
   "source": [
    "# Coffee Break Now! ‚òïÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f072814fc182e2",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks & Deep Learning: The AI \"Brain\" üß†\n",
    "\n",
    "*Neural Networks* are the architectures that enable AI to learn complex relationships between data. Their more advanced forms are called *Deep Learning*.\n",
    "\n",
    "## The Artificial Neuron: The Basic Building Block ‚ö°\n",
    "Just like the human brain has neurons, an Artificial Neural Network has \"artificial neurons\" or \"nodes.\"\n",
    "\n",
    "**What it does**:\n",
    "- It receives inputs (numerical data points or features).\n",
    "- Each input is multiplied by a weight (a number that determines its importance).\n",
    "- All these weighted inputs are summed up.\n",
    "- A bias (another number) is added to this sum.\n",
    "- This result then passes through an activation function (a mathematical squashing function that decides if the neuron \"fires\" or passes information along).\n",
    "- It produces a single output.\n",
    "\n",
    "**Example**: Imagine a small sensor in a drill core that takes readings (inputs) like rock density, magnetic susceptibility, and resistivity. Each reading contributes a certain \"weight\" to the decision. The neuron then processes these combined signals to decide if, say, \"this section of core likely contains a certain mineral\" (its output).\n",
    "```\n",
    "(Input 1) --[Weight 1]-->\n",
    "(Input 2) --[Weight 2]--> [ SUM + BIAS ] --> [ACTIVATION FUNCTION] --> Output\n",
    "(Input 3) --[Weight 3]-->\n",
    "```\n",
    "\n",
    "##  Neural Networks: Connecting the Neurons in Layers üîó\n",
    "\n",
    "Instead of just one neuron, Neural Networks arrange many neurons into layers:\n",
    "- Input Layer: Receives the raw data (e.g., pixel values of an image, geochemical concentrations).\n",
    "- Hidden Layers: One or more layers between the input and output. This is where the magic happens ‚Äì the network learns complex patterns and representations of the data.\n",
    "- Output Layer: Produces the final result (e.g., \"Granite\" or \"Shale\" for classification; a predicted gold grade for regression).\n",
    "\n",
    "**How it Works** (The \"Learning\"):\n",
    "- Forward Pass: Data enters the input layer, flows through the hidden layers, and an output is produced.\n",
    "- Error Calculation: This output is compared to the correct answer (for supervised learning). The difference is the \"error.\"\n",
    "- Backpropagation: The error is then sent backwards through the network. This is the crucial step where the network adjusts its weights and biases slightly in each neuron to reduce the error.\n",
    "- Iteration: This process (forward pass, error, backpropagation, weight adjustment) is repeated thousands, millions, or even billions of times with different data examples.\n",
    "\n",
    "*The \"learning\" is like calibrating each machine in the plant so that the final product (output) is as accurate as possible.*\n",
    "```\n",
    "Input Layer --> [Hidden Layer 1] --> [Hidden Layer 2] --> ... --> Output Layer\n",
    "(Raw Data)       (Learns basic)      (Learns complex)      (Final Prediction)\n",
    "                 (features)          (combinations)\n",
    "```\n",
    "\n",
    "### Deep Learning: More Layers, More Power! üöÄ\n",
    "\n",
    "\"Deep Learning\" is simply a type of Neural Network that has many hidden layers (hence \"deep\").\n",
    "\n",
    "*Why Deep?* Having more layers allows the network to learn increasingly complex and abstract representations of the data.\n",
    "\n",
    "\n",
    "### Key Architectures üèóÔ∏è\n",
    "\n",
    "- **Convolutional Neural Networks** (CNNs). Designed for spatial data, works as a \"slider\" above the image.\n",
    "- **Recurrent Neural Networks** (RNNs) / **Transformers**. Designed for sequential data, especially text and time series.\n",
    "\n",
    "RNNs have a \"memory\" that allows them to process sequences of data, where the order matters (like words in a sentence or measurements over time). Transformers are a more modern and powerful evolution, excelling at understanding long-range dependencies and context in text.  \n",
    "\n",
    "**The Revolution**. Deep Learning became feasible due to:\n",
    "- Big Data: more data -> more examples\n",
    "- Powerful GPUs\n",
    "- New methods: RNNs introduced in 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da021b1031b393",
   "metadata": {},
   "source": [
    "# Modern LLMs and why all of the above matter ü§ñ\n",
    "\n",
    "ChatGPT, Claude, DeepSeek, and other state-of-the-art Large Language Models (LLMs) are all fundamentally based on Deep Learning Networks.\n",
    "They are built upon a particular type of deep learning architecture called the Transformer networks.\n",
    "\n",
    "## Large Language Models (LLMs) \n",
    "\n",
    "These are a specific application of deep learning, typically using the Transformer architecture, trained on colossal amounts of text data. Their \"largeness\" refers to the massive number of parameters (the weights and biases in their neural networks) they contain, which allows them to learn incredibly intricate patterns in human language. Here's a nice [overview from Amazon](https://aws.amazon.com/what-is/large-language-model/).\n",
    "\n",
    "When we talk about ChatGPT or any other LLM:\n",
    "- They are *Deep Learning Networks* because they have many layers and learn complex patterns.\n",
    "- They are primarily *Transformer-based models*, which is the specific architecture enabling their advanced language understanding and generation capabilities.\n",
    "- They are trained using a combination of *unsupervised learning* (predicting the next word in vast amounts of text) and then refined with *reinforcement learning* from human feedback (RLHF) to make their outputs more aligned with human preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc555955d6b1ddc4",
   "metadata": {},
   "source": [
    "# Lunch Break Now! üçî"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
